{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TfP9cumIq3km"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import sympy as sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d-sq0hKhq3kp"
      },
      "outputs": [],
      "source": [
        "# The times used for training (those in the loss function)\n",
        "\n",
        "t_i = [0,20,30]\n",
        "T_i = [3,3,3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "a838Kw_oq3kq"
      },
      "outputs": [],
      "source": [
        "# Linear interpolation for TS\n",
        "num_elements = 13\n",
        "\n",
        "def interpolate_dataframe(df, num_elements):\n",
        "    new_df = pd.DataFrame()  # Create an empty DataFrame to store the interpolated values\n",
        "\n",
        "    # Loop through the columns of the original DataFrame\n",
        "    for col_idx in range(1, len(df.columns) - 1):\n",
        "        col1 = df.iloc[:, col_idx]  # Get the first column\n",
        "        col2 = df.iloc[:, col_idx + 1]  # Get the second column\n",
        "\n",
        "        # Calculate the step size for interpolation\n",
        "        step_size = 1 / (num_elements + 1)\n",
        "\n",
        "        # Create an array of interpolated values between the two columns\n",
        "        interpolated_values = [col1 + i * (col2 - col1) * step_size for i in range(1, num_elements + 1)]\n",
        "\n",
        "        # Concatenate the original column and the interpolated values into a new DataFrame\n",
        "        new_col = pd.concat([col1] + interpolated_values, axis=1)\n",
        "        new_df = pd.concat([new_df, new_col], axis=1)\n",
        "\n",
        "    return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x_AHBO-hq3kr"
      },
      "outputs": [],
      "source": [
        "#Import the Underlying price dataset\n",
        "csv_file = '/content/Neural_martingale/Security_adj.csv'\n",
        "\n",
        "#Use pandas to read the CSV file into a DataFrame\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "#Interpolate the df\n",
        "df_interp = interpolate_dataframe(df, num_elements)\n",
        "\n",
        "# Remove the first column and extract the first 28800 rows and 65 columns for train\n",
        "x_train = df_interp.iloc[:28800, :65]\n",
        "\n",
        "y_train = x_train.iloc[:, t_i].to_numpy()\n",
        "\n",
        "# Print the first few rows of the DataFrame\n",
        "#print(x_train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "osjPKdDIq3ks"
      },
      "outputs": [],
      "source": [
        "#Import the Underlying price dataset\n",
        "csv_file = '/content/Neural_martingale/Strike_adj.csv'\n",
        "\n",
        "#Use pandas to read the CSV file into a DataFrame\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "#Interpolate the df\n",
        "df_interp = interpolate_dataframe(df, num_elements)\n",
        "\n",
        "# Remove the first column and extract the first 28800 rows and 65 columns for train\n",
        "Strike_train = df_interp.iloc[:28800, :65]\n",
        "\n",
        "y_train = np.hstack([y_train, Strike_train.iloc[:, t_i].to_numpy()])       #This will be used to compute the intrinsic value of the option\n",
        "\n",
        "# Print the first few rows of the DataFrame\n",
        "#print(Strike_train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "n9xKuRnnq3kt"
      },
      "outputs": [],
      "source": [
        "#Import the Price dataset\n",
        "csv_file = '/content/Neural_martingale/Price_adj.csv'\n",
        "\n",
        "#Use pandas to read the CSV file into a DataFrame\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "#Interpolate the df\n",
        "df_interp = interpolate_dataframe(df, num_elements)\n",
        "\n",
        "# Remove the first column and extract the first 28800 rows and 65 columns for train\n",
        "Price_train = df_interp.iloc[:28800, :65]\n",
        "\n",
        "\n",
        "y_train = np.hstack([y_train, Price_train.iloc[:, t_i].to_numpy()])\n",
        "\n",
        "# Print the first few rows of the DataFrame\n",
        "#print(Price_train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isyAe781q3kt",
        "outputId": "6cf1c9c0-0a40-4ddf-fe3b-54dbe4d6f6d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 598.        ,  611.57142857,  621.42857143, ...,   14.        ,\n",
              "          19.85714286,   26.85714286],\n",
              "       [ 681.        ,  688.85714286,  694.28571429, ...,   54.        ,\n",
              "          60.42857143,   65.14285714],\n",
              "       [ 704.        ,  704.        ,  703.42857143, ...,   89.        ,\n",
              "          88.57142857,   87.57142857],\n",
              "       ...,\n",
              "       [1325.        , 1285.14285714, 1240.        , ...,    5.        ,\n",
              "           1.71428571,    0.        ],\n",
              "       [1408.        , 1404.28571429, 1388.42857143, ..., 1000.        ,\n",
              "         998.57142857,  984.14285714],\n",
              "       [1391.        , 1409.        , 1411.71428571, ...,    3.        ,\n",
              "           3.42857143,    4.14285714]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Jdntu-tZq3ku"
      },
      "outputs": [],
      "source": [
        "#Define the loss function, then it will be necessary:\n",
        "#Simulate the GBM trajectories.\n",
        "#Simulate the Gaussian variables.\n",
        "\n",
        "mini_batch = 16\n",
        "TSlenght = 65\n",
        "\n",
        "frequencies = np.arange(10,30,1)  #REMEMBER TO THEN ADD THE COSINES FREQUENCIES\n",
        "\n",
        "\n",
        "#FIRST IMPLEMENTATION WITH NO k, FOR EACH t_i WE HAVE ONLY ONE T_i UP TO NOW, ALSO WE ARE TRYING WITH ONLY ONE TIME\n",
        "\n",
        "times = t_i             #the t_i\n",
        "maturities = T_i        #the T_i\n",
        "\n",
        "\n",
        "def quad_var_calc(weights, freq, t, T):\n",
        "    # Function that evaluates the integral of sin^2(...) and 2sin(...)sin(...), that are our frequencies variances\n",
        "\n",
        "    N = len(freq)\n",
        "\n",
        "    covar = tf.TensorArray(dtype = tf.float64, size = N**2)\n",
        "\n",
        "    for k in range(N):\n",
        "        for j in range(N):\n",
        "            if j < k:\n",
        "                covar = covar.write(k*N+j, tf.cast(weights[j]*weights[k],tf.float64)*(T*(tf.sin((2*np.pi*t*(freq[j] - freq[k]))/T)/(freq[j] - freq[k]) - tf.sin((2*np.pi*t*(freq[j] + freq[k]))/T)/(freq[j] + freq[k])))/(4*np.pi))\n",
        "            elif j==k:\n",
        "                covar = covar.write(k*N+j, tf.cast((weights[j]**2),tf.float64)*(t/2 - (T*tf.sin((4*freq[j]*np.pi*t)/T))/(8*freq[j]*np.pi)))\n",
        "\n",
        "    # Extract tensors from the covar tensor array\n",
        "    covar_tensors = covar.stack()\n",
        "\n",
        "    # Compute the sum of elements with k == j only once and the others 2 times\n",
        "    result = 0.0\n",
        "\n",
        "    for k in range(N):\n",
        "        for j in range(N):\n",
        "            if j < k:\n",
        "                result += 2*covar_tensors[k*N+j]\n",
        "            elif j==k:\n",
        "                result += covar_tensors[k*N+j]\n",
        "\n",
        "    return tf.cast(result, tf.float32)\n",
        "\n",
        "\n",
        "def GBM(mu = 0.05, sigma = 0.02, n = 50, dt = 0.001, x0 = 100, batch_size = 64):\n",
        "\n",
        "    #np.random.seed(1)\n",
        "\n",
        "    result = tf.TensorArray(dtype = tf.float32, size = batch_size)\n",
        "\n",
        "    x0 = tf.cast(x0, tf.float32)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        x = np.exp(\n",
        "            (mu - sigma ** 2 / 2) * dt\n",
        "            + sigma * np.random.normal(0, np.sqrt(dt), size=(1, n)).T\n",
        "        )\n",
        "        x = np.vstack([1, x])\n",
        "        #x = np.concatenate(x, axis=0)\n",
        "        partial = x.cumprod(axis=0)\n",
        "        partial = x0[i] * partial[-1]     #return only last point of the GBM\n",
        "        result = result.write(i, partial)\n",
        "    #tf.print(result)\n",
        "\n",
        "    result = result.stack()\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def Martingale(frequencies, weights, eval_time=1, horizon=10, batch_size=64):\n",
        "    # Calculate the integral of the variance function m(t) in order to obtain the variance\n",
        "    variances = []\n",
        "\n",
        "    for j in range(batch_size):\n",
        "        variance_j = quad_var_calc(weights[j], frequencies, eval_time, horizon)\n",
        "        variances.append(variance_j)\n",
        "\n",
        "    variance = tf.stack(variances)\n",
        "\n",
        "    # Define a Normal distribution with TensorFlow\n",
        "    random_value = tf.random.normal([batch_size], mean=0.0, stddev=tf.sqrt(variance), dtype=tf.float32)\n",
        "\n",
        "    return random_value\n",
        "\n",
        "\n",
        "class Q_Loss(tf.keras.losses.Loss):\n",
        "    def __init__(self, times, maturity, lambd = 0, name=\"Q_loss\", **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.times = times\n",
        "        self.maturity = maturity\n",
        "        self.lamb = lambd\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "\n",
        "        N = len(self.times)\n",
        "\n",
        "        #These are arrays of lenght = self.times and size = mini_batch with all the x_i, strike_i and g_i needed\n",
        "        ind = tf.constant(np.linspace(0,N-1,N, dtype=int))\n",
        "        x = tf.transpose(tf.nn.embedding_lookup(tf.transpose(y_true), ind))\n",
        "\n",
        "        ind = tf.constant(np.linspace(N,2*N-1,N, dtype=int))\n",
        "        strike = tf.transpose(tf.nn.embedding_lookup(tf.transpose(y_true), ind))\n",
        "\n",
        "        ind = tf.constant(np.linspace(2*N,3*N-1,N, dtype=int))\n",
        "        g = tf.transpose(tf.nn.embedding_lookup(tf.transpose(y_true), ind))\n",
        "\n",
        "        # convert them to float64\n",
        "        x = tf.cast(x, tf.dtypes.float64)\n",
        "        strike = tf.cast(strike, tf.dtypes.float64)\n",
        "        g = tf.cast(g, tf.dtypes.float64)\n",
        "\n",
        "        #tf.print(strike)\n",
        "        #print(tf.shape(strike))\n",
        "\n",
        "        # Reshape from (64,1) to (64,)\n",
        "        #strike = tf.reshape(strike, (mini_batch,))\n",
        "\n",
        "\n",
        "        max_losses = []  # Create an array to store the maximum loss for each iteration\n",
        "\n",
        "\n",
        "        for i in range(N):       #We'll have to update in order to consider more t_i than just one\n",
        "\n",
        "            # Compute the underlying quantity through GBM\n",
        "\n",
        "            x_star = tf.math.log(GBM(mu = 1.2, sigma = 0.02, n = self.maturity[i], dt = 0.001, x0 = x[:,i], batch_size = mini_batch))\n",
        "            x_star = tf.cast(x_star, tf.dtypes.float64)\n",
        "\n",
        "            # In order to have [0,1,2,...] instead of [[0],[1],[2],...]\n",
        "            x_star = tf.squeeze(x_star)\n",
        "            g = tf.squeeze(g)\n",
        "\n",
        "            # Compute the martingale\n",
        "\n",
        "            mart = Martingale(frequencies, y_pred, self.maturity[i], TSlenght, mini_batch)\n",
        "            mart = tf.cast(mart, tf.dtypes.float64)\n",
        "\n",
        "            # We try as G_i,k the option intrinsic value\n",
        "            loss = tf.square(tf.math.maximum(tf.exp(x_star + mart)-strike[:,i], tf.constant([0], dtype=tf.float64)) - g[:,i])     #+ self.lambd*Martingale(variance = y_pred)  # we'll have to add the regularization term\n",
        "            loss = tf.math.reduce_mean(loss)\n",
        "\n",
        "            # Append the loss to the array of losses\n",
        "            max_losses.append(loss)\n",
        "\n",
        "        # Take the maximum of the loss array\n",
        "        max_loss = tf.reduce_max(max_losses)\n",
        "\n",
        "        return max_loss\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'times': self.times,\n",
        "            'maturity': self.maturity,\n",
        "            'lambda': self.lambd\n",
        "        }\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, **config}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICXAHz3wq3kv"
      },
      "outputs": [],
      "source": [
        "#Define the model, there will be 3 steps:\n",
        "#1) Only Layer of ReLU\n",
        "#2) Layer of ReLU + sin\n",
        "#3) Try the fancy activation functions\n",
        "\n",
        "def custom_activation(x):\n",
        "    return tf.minimum(tf.maximum(x, -0.8), 0.8)\n",
        "\n",
        "# Create a simple model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation='sigmoid', input_shape=(TSlenght,)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(64, activation='sigmoid'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(64, activation='sigmoid'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(32, activation='sigmoid'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(len(frequencies), activation = custom_activation)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddLopLVkq3kv"
      },
      "outputs": [],
      "source": [
        "#Train the model on the dataset and test it.\n",
        "\n",
        "loss_fn = Q_Loss(times, maturities)\n",
        "\n",
        "# Compile the model with our personalized loss function\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss_fn)\n",
        "\n",
        "# Train the model with our personalized loss function\n",
        "model.fit(x_train, y_train, epochs=150, batch_size=mini_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p259xaLaq3kw"
      },
      "outputs": [],
      "source": [
        "# Code for making a prediction\n",
        "\n",
        "predictions = model.predict(x_train)\n",
        "\n",
        "tensor_pred = tf.convert_to_tensor(predictions[0])\n",
        "tensor_pred = tf.reshape(tensor_pred, (1,len(frequencies)))\n",
        "tensor_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXu6kiK6q3k0"
      },
      "outputs": [],
      "source": [
        "# Code for testing the model\n",
        "\n",
        "average, average_mart = 0, 0\n",
        "\n",
        "t = 3\n",
        "N = 1000\n",
        "\n",
        "x_0 = tf.convert_to_tensor([x_train.iloc[0,0]], dtype = tf.float64)\n",
        "\n",
        "# For visualizing the progresses\n",
        "import tqdm\n",
        "it = tqdm.tqdm(total = N)\n",
        "\n",
        "for i in range(N):\n",
        "\n",
        "    # delta between predicted underlying with GBM and actual value\n",
        "\n",
        "    x_star = tf.math.log(GBM(mu = 1.2, sigma = 0.02, n = t, dt = 0.001, x0 = x_0, batch_size = 1))\n",
        "\n",
        "    average += (np.exp(x_star[0].numpy())- x_train.iloc[0,t])**2\n",
        "\n",
        "    mart = Martingale(frequencies, tensor_pred, t, TSlenght, 1)\n",
        "\n",
        "    # delta between predicted underlying with GBM + martingale and actual value\n",
        "\n",
        "    average_mart += (np.exp(x_star[0].numpy() + mart.numpy()) - x_train.iloc[0,t])**2\n",
        "\n",
        "    it.update(1)\n",
        "it.close()\n",
        "\n",
        "average = average/N\n",
        "average_mart = average_mart/N\n",
        "print(average)\n",
        "print(average_mart)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DVb3pSoq3k1",
        "outputId": "0441aeec-9641-4a0d-8f7c-d57728396302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[598]\n",
            "[[600.815125]]\n",
            "[-0.355288118]\n",
            "599.7142857142857\n"
          ]
        }
      ],
      "source": [
        "tf.print(x_0)\n",
        "tf.print(GBM(mu = 1.2, sigma = 0.02, n = 3, dt = 0.001, x0 = x_0, batch_size = 1))\n",
        "tf.print(Martingale(frequencies, tensor_pred, 3, TSlenght, 1))\n",
        "print(x_train.iloc[0,3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WNGnh5Uq3k1",
        "outputId": "d5b96d69-fc27-44dd-a3ff-6bd577efd2a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1438.0952]\n",
            "[1436.9066]\n"
          ]
        }
      ],
      "source": [
        "# Code for testing the model\n",
        "\n",
        "average, average_mart = 0, 0\n",
        "\n",
        "x_0 = tf.convert_to_tensor([x_train.iloc[0,0]], dtype = tf.float64)\n",
        "\n",
        "for i in range(1000):\n",
        "\n",
        "    # delta between predicted underlying with GBM and actual value\n",
        "\n",
        "    x_star = tf.math.log(GBM(mu = 0.05, sigma = 0.02, n = 3, dt = 0.001, x0 = x_0, batch_size = 1))\n",
        "\n",
        "    average += (np.exp(x_star[0].numpy())- x_train.iloc[0,3])**2\n",
        "\n",
        "    mart = Martingale(frequencies, tensor_pred, 3, TSlenght, 1)\n",
        "\n",
        "    # delta between predicted underlying with GBM + martingale and actual value\n",
        "\n",
        "    average_mart += (np.exp(x_star[0].numpy() + mart.numpy()) - x_train.iloc[0,3])**2\n",
        "\n",
        "average = average/1000\n",
        "average_mart = average_mart/1000\n",
        "print(average)\n",
        "print(average_mart)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}