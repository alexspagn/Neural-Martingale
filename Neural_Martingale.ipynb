{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6    7    8    9  ...  86  87  88  89  90  \\\n",
      "0  598  606  619  636  649  655  648  644  652  638  ...   0   0   0   0   0   \n",
      "1  655  648  644  652  638    0    0    0    0    0  ...   0   0   0   0   0   \n",
      "2  649  648  655  633  641  650  654  644  665  678  ...   0   0   0   0   0   \n",
      "3  672  656  634  626  639  664  662  665  664  655  ...   0   0   0   0   0   \n",
      "4  681  685  694  696  704    0    0    0    0    0  ...   0   0   0   0   0   \n",
      "\n",
      "   91  92  93  94  95  \n",
      "0   0   0   0   0   0  \n",
      "1   0   0   0   0   0  \n",
      "2   0   0   0   0   0  \n",
      "3   0   0   0   0   0  \n",
      "4   0   0   0   0   0  \n",
      "\n",
      "[5 rows x 96 columns]\n"
     ]
    }
   ],
   "source": [
    "#Import the Underlying price dataset \n",
    "csv_file = 'Security_adj.csv'\n",
    "\n",
    "#Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Remove the first column and extract the first 80000 rows for train\n",
    "x_train = df.iloc[:80000, 1:]\n",
    "\n",
    "y_train = x_train.iloc[:, 1].to_numpy()\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(x_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1   2   3   4   5   6   7   8   9  ...  86  87  88  89  90  91  92  \\\n",
      "0  14  16  25  38  52  56  51  44  51  37  ...   0   0   0   0   0   0   0   \n",
      "1   9   9  14   8  17   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "2   9  10   8  11   9   8   8   9   6   5  ...   0   0   0   0   0   0   0   \n",
      "3  37  49  71  78  67  44  46  44  44  53  ...   0   0   0   0   0   0   0   \n",
      "4  54  57  65  66  74   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "\n",
      "   93  94  95  \n",
      "0   0   0   0  \n",
      "1   0   0   0  \n",
      "2   0   0   0  \n",
      "3   0   0   0  \n",
      "4   0   0   0  \n",
      "\n",
      "[5 rows x 96 columns]\n"
     ]
    }
   ],
   "source": [
    "#Import the Price dataset \n",
    "csv_file = 'Price_adj.csv'\n",
    "\n",
    "#Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Remove the first column\n",
    "Price_train = df.iloc[:80000, 1:]\n",
    "\n",
    "y_train = np.vstack([y_train, Price_train.iloc[:, 1].to_numpy()])\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(Price.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definire la loss function, quindi sar√† necessario:\n",
    "#Simulare le traiettorie del GBM\n",
    "#Simulare le variabili Gaussiane\n",
    "\n",
    "frequencies = [1,2,3,4,5]   #REMEMBER TO THEN ADD THE COSINES FREQUENCIES\n",
    "\n",
    "\n",
    "#FIRST IMPLEMENTATION WITH NO k, FOR EACH t_i WE HAVE ONLY ONE T_i UP TO NOW, ALSO WE ARE TRYING WITH ONLY ONE TIME\n",
    "\n",
    "times = [1]             #the t_i\n",
    "maturities = [3]        #the T_i\n",
    "\n",
    "\n",
    "def quad_var_calc(freq, eval_time, horizon):\n",
    "    # Function that evaluates the integral of sin^2(...), that is our single frequency variance\n",
    "    result = eval_time/2 - (horizon*np.sin((4*freq*np.pi*eval_time)/horizon))/(8*freq*np.pi)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def GBM(mu = 1, sigma = 1, n = 50, dt = 0.01, x0 = 100):\n",
    "\n",
    "    #np.random.seed(1)\n",
    "\n",
    "    result = np.zeros([32])\n",
    "    for i in range(32):\n",
    "        x = np.exp(\n",
    "            (mu - sigma ** 2 / 2) * dt\n",
    "            + sigma * np.random.normal(0, np.sqrt(dt), size=(1, n)).T\n",
    "        )\n",
    "        x = np.vstack([1, x])\n",
    "        x = np.concatenate(x, axis=0)\n",
    "        partial = x0[i].numpy() * x.cumprod(axis=0)         #we'll have to fix the numpy conversion\n",
    "        result[i] = partial[-1]     #return only last point of the GBM\n",
    "\n",
    "    result = tf.convert_to_tensor(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def Martingale(frequencies, weights, eval_time = 1, horizon = 10):\n",
    "\n",
    "    #Calculate the integral of the variance function m(t) in order to obtain the variance\n",
    "    variance = 0\n",
    "    quad_var = np.zeros(len(frequencies)*32).reshape(32,len(frequencies))\n",
    "\n",
    "    #we calculate separately the variancies and multiply them by the weights tensor, then we'll sum the frequencies (in the Q_loss code)\n",
    "    for i in range(len(frequencies)):\n",
    "        for j in range(32):\n",
    "            quad_var[j][i] = quad_var_calc(frequencies[i], eval_time, horizon)\n",
    "\n",
    "\n",
    "    variance = tf.keras.layers.Multiply()([weights,quad_var.reshape(32, len(frequencies))])\n",
    "    #tf.print(variance)\n",
    "\n",
    "    # Define a Normal distribution with tensorflow\n",
    "    random_value = tf.random.normal([32,len(frequencies)], mean=0.0, stddev=tf.sqrt(variance), dtype=tf.dtypes.float32)\n",
    "    #tf.print(random_value)\n",
    "    return random_value\n",
    "\n",
    "\n",
    "class Q_Loss(tf.keras.losses.Loss):\n",
    "    def __init__(self, times, maturity, lambd = 0, name=\"Q_loss\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.times = times\n",
    "        self.maturity = maturity\n",
    "        self.lamb = lambd\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        #These are arrays of lenght = self.times with all the x_i and g_i needed\n",
    "        ind = tf.constant([0])\n",
    "        x = tf.transpose(tf.nn.embedding_lookup(tf.transpose(y_true), ind))\n",
    "\n",
    "        ind = tf.constant([1])\n",
    "        g = tf.transpose(tf.nn.embedding_lookup(tf.transpose(y_true), ind))\n",
    "\n",
    "        # convert them to float64\n",
    "        x = tf.cast(x, tf.dtypes.float64)\n",
    "        g = tf.cast(g, tf.dtypes.float64)\n",
    "\n",
    "        # define the arrays of all the i underlying values\n",
    "        x_star = np.zeros(len(self.times))\n",
    "        mart = np.zeros(len(self.times))\n",
    "\n",
    "        #for i in range(len(self.times)):       #We'll have to update in order to consider more t_i than just one\n",
    "\n",
    "        # compute the underlying quantity\n",
    "        x_star = tf.math.log(GBM(mu = 1, sigma = 1, n = self.maturity[0], dt = 0.01, x0 = x))\n",
    "        x_star = tf.cast(x_star, tf.dtypes.float64)\n",
    "        \n",
    "        mart = Martingale(frequencies, y_pred, self.maturity[0], 96)\n",
    "        mart = tf.math.reduce_sum(mart,1)\n",
    "        mart = tf.cast(mart, tf.dtypes.float64)\n",
    "        \n",
    "        loss = tf.square(tf.exp(x_star + mart) - g)     #+ self.lambd*Martingale(variance = y_pred)  # we'll have to add the regularization term\n",
    "        #tf.print(tf.math.reduce_mean(loss))\n",
    "        return tf.math.reduce_mean(loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'times': self.times,\n",
    "            'maturity': self.maturity,\n",
    "            'lambda': self.lambd\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, **config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definire il modello, ci saranno 3 steps:\n",
    "#1) Solo Layer di ReLU\n",
    "#2) Layer di ReLU + sin\n",
    "#3) Provare le fancy activation functions\n",
    "\n",
    "# Create a simple model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='sigmoid', input_shape=(96,)),\n",
    "    tf.keras.layers.Dense(512, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(512, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(256, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')  # Softmax activation for probability distribution\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainare il modello sul dataset e testarlo\n",
    "\n",
    "loss_fn = Q_Loss(times, maturities)\n",
    "\n",
    "# Compile the model with your personalized loss function\n",
    "# when we'll have a working loss function we'll have to remove the run_eagerly and rewrite properly the GBM function\n",
    "# since it is used in order to convert x0 to numpy and then convert it back, but we can do this all only with tensors\n",
    "# speeding up a lot the calculations\n",
    "model.compile(optimizer='adam', loss=loss_fn, run_eagerly=True)\n",
    "\n",
    "# Train the model with your personalized loss function\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
