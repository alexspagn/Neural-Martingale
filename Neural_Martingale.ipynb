{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The times used for training (those in the loss function)\n",
    "\n",
    "t_i, T_i = 0, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0     1     2     3     4     5     6     7     8     9  ...  42  43  \\\n",
      "0   598   606   619   636   649   655   648   644   652   638  ...   0   0   \n",
      "1   681   685   694   696   704     0     0     0     0     0  ...   0   0   \n",
      "2   704   707   700   724   731   743   755   745   740   731  ...   0   0   \n",
      "3  1101  1108  1101  1119  1130  1094  1104  1118  1119  1092  ...   0   0   \n",
      "4  1107  1132  1148  1166  1174  1164  1125  1081  1084  1098  ...   0   0   \n",
      "\n",
      "   44  45  46  47  48  49  50  51  \n",
      "0   0   0   0   0   0   0   0   0  \n",
      "1   0   0   0   0   0   0   0   0  \n",
      "2   0   0   0   0   0   0   0   0  \n",
      "3   0   0   0   0   0   0   0   0  \n",
      "4   0   0   0   0   0   0   0   0  \n",
      "\n",
      "[5 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "#Import the Underlying price dataset \n",
    "csv_file = 'Security_adj.csv'\n",
    "\n",
    "#Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Remove the first column and extract the first 80000 rows for train\n",
    "x_train = df.iloc[:28800, 1:]\n",
    "\n",
    "y_train = x_train.iloc[:, t_i].to_numpy()\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(x_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0     1     2     3     4     5     6     7     8     9  ...  42  43  \\\n",
      "0   600   600   600   600   600   600   600   600   600   600  ...   0   0   \n",
      "1   630   630   630   630   630     0     0     0     0     0  ...   0   0   \n",
      "2   620   620   620   620   620   620   620   620   620   620  ...   0   0   \n",
      "3  1005  1005  1005  1005  1005  1005  1005  1005  1005  1005  ...   0   0   \n",
      "4   850   850   850   850   850   850   850   850   850   850  ...   0   0   \n",
      "\n",
      "   44  45  46  47  48  49  50  51  \n",
      "0   0   0   0   0   0   0   0   0  \n",
      "1   0   0   0   0   0   0   0   0  \n",
      "2   0   0   0   0   0   0   0   0  \n",
      "3   0   0   0   0   0   0   0   0  \n",
      "4   0   0   0   0   0   0   0   0  \n",
      "\n",
      "[5 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "#Import the Underlying price dataset \n",
    "csv_file = 'Strike_adj.csv'\n",
    "\n",
    "#Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Remove the first column and extract the first 80000 rows for train\n",
    "Strike_train = df.iloc[:28800, 1:]\n",
    "\n",
    "y_train = np.vstack([y_train, Strike_train.iloc[:, t_i].to_numpy()])       #This will be used to compute the intrinsic value of the option\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(Strike_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6    7    8    9  ...  42  43  44  45  46  \\\n",
      "0   14   16   25   38   52   56   51   44   51   37  ...   0   0   0   0   0   \n",
      "1   54   57   65   66   74    0    0    0    0    0  ...   0   0   0   0   0   \n",
      "2   89   92   84  109  113  125  135  127  119  111  ...   0   0   0   0   0   \n",
      "3  175  182  178  189  194  168  176  187  186  166  ...   0   0   0   0   0   \n",
      "4  267  290  300  318  327  316  274  234  237  249  ...   0   0   0   0   0   \n",
      "\n",
      "   47  48  49  50  51  \n",
      "0   0   0   0   0   0  \n",
      "1   0   0   0   0   0  \n",
      "2   0   0   0   0   0  \n",
      "3   0   0   0   0   0  \n",
      "4   0   0   0   0   0  \n",
      "\n",
      "[5 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "#Import the Price dataset \n",
    "csv_file = 'Price_adj.csv'\n",
    "\n",
    "#Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Remove the first column\n",
    "Price_train = df.iloc[:28800, 1:]\n",
    "\n",
    "y_train = np.vstack([y_train, Price_train.iloc[:, t_i].to_numpy()])\n",
    "y_train = y_train.T\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(Price_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 598,  600,   14],\n",
       "       [ 681,  630,   54],\n",
       "       [ 704,  620,   89],\n",
       "       ...,\n",
       "       [1325, 1430,    5],\n",
       "       [1408,  400, 1000],\n",
       "       [1391, 1675,    3]], dtype=int64)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the loss function, then it will be necessary:\n",
    "#Simulate the GBM trajectories.\n",
    "#Simulate the Gaussian variables.\n",
    "\n",
    "mini_batch = 64\n",
    "TSlenght = 52\n",
    "\n",
    "frequencies = np.arange(1,30,1)  #REMEMBER TO THEN ADD THE COSINES FREQUENCIES\n",
    "\n",
    "\n",
    "#FIRST IMPLEMENTATION WITH NO k, FOR EACH t_i WE HAVE ONLY ONE T_i UP TO NOW, ALSO WE ARE TRYING WITH ONLY ONE TIME\n",
    "\n",
    "times = [t_i]             #the t_i\n",
    "maturities = [T_i]        #the T_i\n",
    "\n",
    "'''\n",
    "def quad_var_calc(freq, eval_time, horizon):\n",
    "    # Function that evaluates the integral of sin^2(...), that is our single frequency variance\n",
    "    result = eval_time/2 - (horizon*np.sin((4*freq*np.pi*eval_time)/horizon))/(8*freq*np.pi)\n",
    "\n",
    "    return result\n",
    "'''\n",
    "\n",
    "def quad_var_calc(weights, t, T, frequencies):\n",
    "    num_steps = 100  # Number of steps for numerical integration\n",
    "    delta_s = t / num_steps  # Step size\n",
    "\n",
    "    N = len(frequencies)\n",
    "\n",
    "    integral_sum = tf.constant(0.0, dtype=tf.float32)\n",
    "\n",
    "    for i in range(1, num_steps + 1):\n",
    "        s = i * delta_s\n",
    "\n",
    "        # Calculate the term in the sum for the given s\n",
    "        term = tf.reduce_sum(\n",
    "            tf.multiply(weights, tf.sin((2 * np.pi / T) * tf.range(1, N+1, dtype=tf.float32) * s))\n",
    "        )\n",
    "\n",
    "        integral_sum += term**2 * delta_s\n",
    "\n",
    "    integral_approx = integral_sum\n",
    "\n",
    "    return integral_approx\n",
    "\n",
    "\n",
    "def GBM(mu = 0.05, sigma = 0.2, n = 50, dt = 0.001, x0 = 100, batch_size = 64):\n",
    "\n",
    "    #np.random.seed(1)\n",
    "\n",
    "    result = tf.TensorArray(dtype = tf.float32, size = batch_size)\n",
    "\n",
    "    x0 = tf.cast(x0, tf.float32)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        x = np.exp(\n",
    "            (mu - sigma ** 2 / 2) * dt\n",
    "            + sigma * np.random.normal(0, np.sqrt(dt), size=(1, n)).T\n",
    "        )\n",
    "        x = np.vstack([1, x])\n",
    "        #x = np.concatenate(x, axis=0)\n",
    "        partial = x.cumprod(axis=0) \n",
    "        partial = x0[i] * partial[-1]     #return only last point of the GBM\n",
    "        result = result.write(i, partial)\n",
    "    #tf.print(result)\n",
    "\n",
    "    result = result.stack()\n",
    "\n",
    "    return result\n",
    "\n",
    "'''\n",
    "def Martingale(frequencies, weights, eval_time=1, horizon=10, batch_size=64):\n",
    "\n",
    "    # Calculate the integral of the variance function m(t) in order to obtain the variance\n",
    "    quad_var = tf.zeros([batch_size, len(frequencies)])\n",
    "\n",
    "    # We calculate separately the variances and multiply them by the weights tensor\n",
    "    for i in range(len(frequencies)):\n",
    "        for j in range(batch_size):\n",
    "            quad_var = tf.tensor_scatter_nd_update(quad_var, [[j, i]], [quad_var_calc(frequencies[i], eval_time, horizon)])\n",
    "\n",
    "    variance = tf.multiply(weights, quad_var)\n",
    "\n",
    "    # Define a Normal distribution with TensorFlow\n",
    "    random_value = tf.random.normal([batch_size, len(frequencies)], mean=0.0, stddev=tf.sqrt(variance), dtype=tf.dtypes.float32)\n",
    "\n",
    "    return random_value\n",
    "'''\n",
    "\n",
    "def Martingale(frequencies, weights, eval_time=1, horizon=10, batch_size=64):\n",
    "    # Calculate the integral of the variance function m(t) in order to obtain the variance\n",
    "    variances = []\n",
    "    \n",
    "    for j in range(batch_size):\n",
    "        variance_j = quad_var_calc(weights[j], eval_time, horizon, frequencies)\n",
    "        variances.append(variance_j)\n",
    "    \n",
    "    variance = tf.stack(variances)\n",
    "    \n",
    "    # Define a Normal distribution with TensorFlow\n",
    "    random_value = tf.random.normal([batch_size], mean=0.0, stddev=tf.sqrt(variance), dtype=tf.float32)\n",
    "\n",
    "    return random_value\n",
    "\n",
    "\n",
    "class Q_Loss(tf.keras.losses.Loss):\n",
    "    def __init__(self, times, maturity, lambd = 0, name=\"Q_loss\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.times = times\n",
    "        self.maturity = maturity\n",
    "        self.lamb = lambd\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        #These are arrays of lenght = self.times and size = mini_batch with all the x_i, strike_i and g_i needed\n",
    "        ind = tf.constant([0])\n",
    "        x = tf.transpose(tf.nn.embedding_lookup(tf.transpose(y_true), ind))\n",
    "\n",
    "        ind = tf.constant([1])\n",
    "        strike = tf.transpose(tf.nn.embedding_lookup(tf.transpose(y_true), ind))\n",
    "\n",
    "        \n",
    "        ind = tf.constant([2])\n",
    "        g = tf.transpose(tf.nn.embedding_lookup(tf.transpose(y_true), ind))\n",
    "\n",
    "        # convert them to float64\n",
    "        x = tf.cast(x, tf.dtypes.float64)\n",
    "        strike = tf.cast(strike, tf.dtypes.float64)\n",
    "        g = tf.cast(g, tf.dtypes.float64)\n",
    "\n",
    "        # Reshape from (64,1) to (64,)\n",
    "        strike = tf.reshape(strike, (mini_batch,))\n",
    "\n",
    "\n",
    "        #for i in range(len(self.times)):       #We'll have to update in order to consider more t_i than just one\n",
    "\n",
    "        # Compute the underlying quantity through GBM\n",
    "\n",
    "        x_star = tf.math.log(GBM(mu = 0.05, sigma = 0.2, n = self.maturity[0], dt = 0.001, x0 = x, batch_size = mini_batch))\n",
    "        x_star = tf.cast(x_star, tf.dtypes.float64)\n",
    "\n",
    "        # In order to have [0,1,2,...] instead of [[0],[1],[2],...]\n",
    "        x_star = tf.squeeze(x_star)\n",
    "        g = tf.squeeze(g)\n",
    "\n",
    "        # Compute the martingale\n",
    "        \n",
    "        mart = Martingale(frequencies, y_pred, self.maturity[0], TSlenght, mini_batch)\n",
    "        #mart = tf.math.reduce_sum(mart,1)       #Sum all the frequencies together\n",
    "        mart = tf.cast(mart, tf.dtypes.float64)\n",
    "\n",
    "        # We try as G_i,k the option intrinsic value\n",
    "        loss = tf.square(tf.math.maximum(tf.exp(x_star + mart)-strike, tf.constant([0], dtype=tf.float64)) - g)     #+ self.lambd*Martingale(variance = y_pred)  # we'll have to add the regularization term\n",
    "        #tf.print(tf.math.maximum(tf.exp(x_star + mart)-strike, tf.constant([0], dtype=tf.float64)) - g)\n",
    "        return tf.math.reduce_mean(loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'times': self.times,\n",
    "            'maturity': self.maturity,\n",
    "            'lambda': self.lambd\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, **config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model, there will be 3 steps:\n",
    "#1) Only Layer of ReLU\n",
    "#2) Layer of ReLU + sin\n",
    "#3) Try the fancy activation functions\n",
    "\n",
    "def custom_activation(x):\n",
    "    return tf.minimum(tf.maximum(x, -0.8), 0.8)\n",
    "\n",
    "# Create a simple model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='sigmoid', input_shape=(TSlenght,)),\n",
    "    tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(32, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(len(frequencies), activation = custom_activation)  # Softmax activation for probability distribution\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 88/450 [====>.........................] - ETA: 4:04 - loss: 866559.0625"
     ]
    }
   ],
   "source": [
    "#Train the model on the dataset and test it.\n",
    "\n",
    "loss_fn = Q_Loss(times, maturities)\n",
    "\n",
    "# Compile the model with our personalized loss function\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss_fn)\n",
    "\n",
    "# Train the model with our personalized loss function\n",
    "model.fit(x_train, y_train, epochs=30, batch_size=mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900/900 [==============================] - 1s 701us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 29), dtype=float32, numpy=\n",
       "array([[ 0.8       , -0.49280283,  0.4969293 ,  0.8       , -0.3966284 ,\n",
       "         0.60507387, -0.8       , -0.33343518, -0.03114109, -0.12552714,\n",
       "        -0.7483919 ,  0.8       , -0.18543218,  0.8       , -0.06704858,\n",
       "         0.0066129 , -0.09024712, -0.08749251,  0.15876706, -0.02691003,\n",
       "        -0.8       , -0.22111495,  0.43936592,  0.39044264,  0.45710498,\n",
       "        -0.38128135, -0.1811505 , -0.12903973,  0.13731556]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for making a prediction\n",
    "\n",
    "predictions = model.predict(x_train)\n",
    "\n",
    "tensor_pred = tf.convert_to_tensor(predictions[0])\n",
    "tensor_pred = tf.reshape(tensor_pred, (1,len(frequencies)))\n",
    "tensor_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1438.0201]\n",
      "[1459.542]\n"
     ]
    }
   ],
   "source": [
    "# Code for testing the model\n",
    "\n",
    "average, average_mart = 0, 0\n",
    "\n",
    "x_0 = tf.convert_to_tensor([x_train.iloc[0,0]], dtype = tf.float64)\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    # delta between predicted underlying with GBM and actual value\n",
    "\n",
    "    x_star = tf.math.log(GBM(mu = 0.05, sigma = 0.02, n = 3, dt = 0.001, x0 = x_0, batch_size = 1))\n",
    "\n",
    "    average += (np.exp(x_star[0].numpy())- x_train.iloc[0,3])**2\n",
    "\n",
    "    mart = Martingale(frequencies, tensor_pred, 3, TSlenght, 1)\n",
    "\n",
    "    # delta between predicted underlying with GBM + martingale and actual value\n",
    "\n",
    "    average_mart += (np.exp(x_star[0].numpy() + mart.numpy()) - x_train.iloc[0,3])**2\n",
    "\n",
    "average = average/1000\n",
    "average_mart = average_mart/1000\n",
    "print(average)\n",
    "print(average_mart)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
