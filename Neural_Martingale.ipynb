{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The times used for training (those in the loss function)\n",
    "\n",
    "t_i, T_i = 0, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6    7    8    9  ...  86  87  88  89  90  \\\n",
      "0  598  606  619  636  649  655  648  644  652  638  ...   0   0   0   0   0   \n",
      "1  655  648  644  652  638    0    0    0    0    0  ...   0   0   0   0   0   \n",
      "2  649  648  655  633  641  650  654  644  665  678  ...   0   0   0   0   0   \n",
      "3  672  656  634  626  639  664  662  665  664  655  ...   0   0   0   0   0   \n",
      "4  681  685  694  696  704    0    0    0    0    0  ...   0   0   0   0   0   \n",
      "\n",
      "   91  92  93  94  95  \n",
      "0   0   0   0   0   0  \n",
      "1   0   0   0   0   0  \n",
      "2   0   0   0   0   0  \n",
      "3   0   0   0   0   0  \n",
      "4   0   0   0   0   0  \n",
      "\n",
      "[5 rows x 96 columns]\n"
     ]
    }
   ],
   "source": [
    "#Import the Underlying price dataset \n",
    "csv_file = 'Security_adj.csv'\n",
    "\n",
    "#Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Remove the first column and extract the first 80000 rows for train\n",
    "x_train = df.iloc[:70016, 1:]\n",
    "\n",
    "y_train = x_train.iloc[:, t_i].to_numpy()\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(x_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6    7    8    9  ...  86  87  88  89  90  \\\n",
      "0  600  600  600  600  600  600  600  600  600  600  ...   0   0   0   0   0   \n",
      "1  655  655  655  655  655    0    0    0    0    0  ...   0   0   0   0   0   \n",
      "2  550  550  550  550  550  550  550  550  550  550  ...   0   0   0   0   0   \n",
      "3  710  710  710  710  710  710  710  710  710  710  ...   0   0   0   0   0   \n",
      "4  630  630  630  630  630    0    0    0    0    0  ...   0   0   0   0   0   \n",
      "\n",
      "   91  92  93  94  95  \n",
      "0   0   0   0   0   0  \n",
      "1   0   0   0   0   0  \n",
      "2   0   0   0   0   0  \n",
      "3   0   0   0   0   0  \n",
      "4   0   0   0   0   0  \n",
      "\n",
      "[5 rows x 96 columns]\n"
     ]
    }
   ],
   "source": [
    "#Import the Underlying price dataset \n",
    "csv_file = 'Strike_adj.csv'\n",
    "\n",
    "#Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Remove the first column and extract the first 80000 rows for train\n",
    "Strike_train = df.iloc[:70016, 1:]\n",
    "\n",
    "y_train = np.vstack([y_train, Strike_train.iloc[:, t_i].to_numpy()])       #This will be used to compute the intrinsic value of the option\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(Strike_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1   2   3   4   5   6   7   8   9  ...  86  87  88  89  90  91  92  \\\n",
      "0  14  16  25  38  52  56  51  44  51  37  ...   0   0   0   0   0   0   0   \n",
      "1   9   9  14   8  17   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "2   9  10   8  11   9   8   8   9   6   5  ...   0   0   0   0   0   0   0   \n",
      "3  37  49  71  78  67  44  46  44  44  53  ...   0   0   0   0   0   0   0   \n",
      "4  54  57  65  66  74   0   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "\n",
      "   93  94  95  \n",
      "0   0   0   0  \n",
      "1   0   0   0  \n",
      "2   0   0   0  \n",
      "3   0   0   0  \n",
      "4   0   0   0  \n",
      "\n",
      "[5 rows x 96 columns]\n"
     ]
    }
   ],
   "source": [
    "#Import the Price dataset \n",
    "csv_file = 'Price_adj.csv'\n",
    "\n",
    "#Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Remove the first column\n",
    "Price_train = df.iloc[:70016, 1:]\n",
    "\n",
    "y_train = np.vstack([y_train, Price_train.iloc[:, t_i].to_numpy()])\n",
    "y_train = y_train.T\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(Price_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 598,  600,   14],\n",
       "       [ 655,  655,    9],\n",
       "       [ 649,  550,    9],\n",
       "       ...,\n",
       "       [1888, 1910,    9],\n",
       "       [1986, 1925,  135],\n",
       "       [ 896, 1450,    0]], dtype=int64)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definire la loss function, quindi sar√† necessario:\n",
    "#Simulare le traiettorie del GBM\n",
    "#Simulare le variabili Gaussiane\n",
    "\n",
    "mini_batch = 64\n",
    "\n",
    "frequencies = [1,2,3,4,5]   #REMEMBER TO THEN ADD THE COSINES FREQUENCIES\n",
    "\n",
    "\n",
    "#FIRST IMPLEMENTATION WITH NO k, FOR EACH t_i WE HAVE ONLY ONE T_i UP TO NOW, ALSO WE ARE TRYING WITH ONLY ONE TIME\n",
    "\n",
    "times = [t_i]             #the t_i\n",
    "maturities = [T_i]        #the T_i\n",
    "\n",
    "\n",
    "def quad_var_calc(freq, eval_time, horizon):\n",
    "    # Function that evaluates the integral of sin^2(...), that is our single frequency variance\n",
    "    result = eval_time/2 - (horizon*np.sin((4*freq*np.pi*eval_time)/horizon))/(8*freq*np.pi)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# In order to make TensorFlow able to correctly convert a function that creates a state (thus, that uses Variables)\n",
    "# we have to break the function scope, declaring the variables outside of the function. This because we don't want\n",
    "# to work in eager mode, which will cost us more time for the training\n",
    "result = None\n",
    "\n",
    "@tf.function\n",
    "def GBM(mu = 1, sigma = 1, n = 50, dt = 0.001, x0 = 100, batch_size = 64):\n",
    "\n",
    "    #np.random.seed(1)\n",
    "    global result\n",
    "    if result is None:\n",
    "        result = tf.Variable(tf.zeros(shape=[batch_size], dtype=tf.dtypes.float64))\n",
    "    for i in range(batch_size):\n",
    "        x = np.exp(\n",
    "            (mu - sigma ** 2 / 2) * dt\n",
    "            + sigma * np.random.normal(0, np.sqrt(dt), size=(1, n)).T\n",
    "        )\n",
    "        x = np.vstack([1, x])\n",
    "        #x = np.concatenate(x, axis=0)\n",
    "        partial = x.cumprod(axis=0) \n",
    "        partial = x0[i] * partial[-1]     #return only last point of the GBM\n",
    "        result[i].assign(partial)\n",
    "    #tf.print(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def Martingale(frequencies, weights, eval_time = 1, horizon = 10, batch_size = 64):\n",
    "\n",
    "    #Calculate the integral of the variance function m(t) in order to obtain the variance\n",
    "    variance = 0\n",
    "    quad_var = np.zeros(len(frequencies)*batch_size).reshape(batch_size,len(frequencies))\n",
    "\n",
    "    #we calculate separately the variancies and multiply them by the weights tensor, then we'll sum the frequencies (in the Q_loss code)\n",
    "    for i in range(len(frequencies)):\n",
    "        for j in range(batch_size):\n",
    "            quad_var[j][i] = quad_var_calc(frequencies[i], eval_time, horizon)\n",
    "    #print(quad_var)\n",
    "\n",
    "    variance = tf.keras.layers.Multiply()([weights,quad_var.reshape(batch_size, len(frequencies))])\n",
    "    #tf.print(variance)\n",
    "\n",
    "    # Define a Normal distribution with tensorflow\n",
    "    random_value = tf.random.normal([batch_size,len(frequencies)], mean=0.0, stddev=tf.sqrt(variance), dtype=tf.dtypes.float32)\n",
    "    #tf.print(random_value)\n",
    "    return random_value\n",
    "\n",
    "\n",
    "class Q_Loss(tf.keras.losses.Loss):\n",
    "    def __init__(self, times, maturity, lambd = 0, name=\"Q_loss\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.times = times\n",
    "        self.maturity = maturity\n",
    "        self.lamb = lambd\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        #These are arrays of lenght = self.times and size = mini_batch with all the x_i, strike_i and g_i needed\n",
    "        ind = tf.constant([0])\n",
    "        x = tf.transpose(tf.nn.embedding_lookup(tf.transpose(y_true), ind))\n",
    "        #tf.print(x)\n",
    "        ind = tf.constant([1])\n",
    "        strike = tf.transpose(tf.nn.embedding_lookup(tf.transpose(y_true), ind))\n",
    "\n",
    "        \n",
    "        ind = tf.constant([2])\n",
    "        g = tf.transpose(tf.nn.embedding_lookup(tf.transpose(y_true), ind))\n",
    "        #tf.print(strike)\n",
    "        # convert them to float64\n",
    "        x = tf.cast(x, tf.dtypes.float64)\n",
    "        strike = tf.cast(strike, tf.dtypes.float64)\n",
    "        g = tf.cast(g, tf.dtypes.float64)\n",
    "\n",
    "        # Reshape from (64,1) to (64,)\n",
    "        strike = tf.reshape(strike, (mini_batch,))\n",
    "\n",
    "\n",
    "        #for i in range(len(self.times)):       #We'll have to update in order to consider more t_i than just one\n",
    "\n",
    "        # compute the underlying quantity\n",
    "        x_star = tf.math.log(GBM(mu = 1, sigma = 1, n = self.maturity[0], dt = 0.001, x0 = x, batch_size = mini_batch))\n",
    "        x_star = tf.cast(x_star, tf.dtypes.float64)\n",
    "        \n",
    "        mart = Martingale(frequencies, y_pred, self.maturity[0], 96, mini_batch)\n",
    "        mart = tf.math.reduce_sum(mart,1)       #Sum all the frequencies together\n",
    "        mart = tf.cast(mart, tf.dtypes.float64)\n",
    "        #tf.print(x_star)\n",
    "\n",
    "        # We try as G_i,k the option intrinsic value\n",
    "        loss = tf.square(tf.math.maximum(tf.exp(x_star + mart)-strike, tf.constant([0], dtype=tf.float64)) - g)     #+ self.lambd*Martingale(variance = y_pred)  # we'll have to add the regularization term\n",
    "\n",
    "        return tf.math.reduce_mean(loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'times': self.times,\n",
    "            'maturity': self.maturity,\n",
    "            'lambda': self.lambd\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, **config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definire il modello, ci saranno 3 steps:\n",
    "#1) Solo Layer di ReLU\n",
    "#2) Layer di ReLU + sin\n",
    "#3) Provare le fancy activation functions\n",
    "\n",
    "# Create a simple model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='sigmoid', input_shape=(96,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')  # Softmax activation for probability distribution\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 194519.0156\n",
      "Epoch 2/10\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 183332.0625\n",
      "Epoch 3/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 179248.8125\n",
      "Epoch 4/10\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 180974.0625\n",
      "Epoch 5/10\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 180842.5781\n",
      "Epoch 6/10\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 182397.8125\n",
      "Epoch 7/10\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 179949.1562\n",
      "Epoch 8/10\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 181112.2500\n",
      "Epoch 9/10\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 181040.3750\n",
      "Epoch 10/10\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 181467.7656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x213a17033a0>"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trainare il modello sul dataset e testarlo\n",
    "\n",
    "loss_fn = Q_Loss(times, maturities)\n",
    "\n",
    "# Compile the model with our personalized loss function\n",
    "\n",
    "# when we'll have a working loss function we'll have to remove the run_eagerly and rewrite properly the GBM function\n",
    "# since it is used in order to convert x0 to numpy and then convert it back, but we can do this all only with tensors\n",
    "# speeding up a lot the calculations\n",
    "model.compile(optimizer='adam', loss=loss_fn, run_eagerly=False)\n",
    "\n",
    "# Train the model with your personalized loss function\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2188/2188 [==============================] - 2s 775us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_pred = tf.convert_to_tensor(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       "array([[9.9999642e-01, 1.2711969e-06, 7.7364047e-07, 7.1407646e-07,\n",
       "        8.2845776e-07]], dtype=float32)>"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_pred = tf.reshape(tensor_pred, (1,5))\n",
    "tensor_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float64, numpy=array([598.])>"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.convert_to_tensor([x_train.iloc[0,0]], dtype = tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star = tf.math.log(GBM(mu = 1, sigma = 1, n = 2, dt = 0.001, x0 = tf.convert_to_tensor([x_train.iloc[0,0]], dtype = tf.float64), batch_size = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-25.91629691108608"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delta between predicted underlying with GBM and actual value\n",
    "\n",
    "np.exp(x_star[0].numpy())- x_train.iloc[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-17.053284], dtype=float32)"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mart = Martingale(frequencies, tensor_pred, 2, 96, 1)\n",
    "\n",
    "# delta between predicted underlying with GBM + martingale and actual value\n",
    "\n",
    "np.exp(x_star[0].numpy() + tf.math.reduce_sum(mart,1).numpy()) - x_train.iloc[0,2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
